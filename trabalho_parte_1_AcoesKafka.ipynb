{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q kafka-python\n",
        "!pip install confluent_kafka\n",
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "V8jV44UO5tkI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c2a7e21-78f9-481a-8052-929baf3cec5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: confluent_kafka in /usr/local/lib/python3.10/dist-packages (2.5.0)\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.2)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Baixar e configurar Kafka\n",
        "!wget https://archive.apache.org/dist/kafka/2.8.0/kafka_2.12-2.8.0.tgz\n",
        "!tar -xzf kafka_2.12-2.8.0.tgz\n",
        "\n",
        "# Iniciar Zookeeper\n",
        "!kafka_2.12-2.8.0/bin/zookeeper-server-start.sh -daemon kafka_2.12-2.8.0/config/zookeeper.properties\n",
        "\n",
        "# Iniciar Kafka\n",
        "!kafka_2.12-2.8.0/bin/kafka-server-start.sh -daemon kafka_2.12-2.8.0/config/server.properties\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9TXrBwP5vQ2",
        "outputId": "0b6ecfcd-4d90-4d2c-b55a-f2237b683050"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-08-26 20:46:12--  https://archive.apache.org/dist/kafka/2.8.0/kafka_2.12-2.8.0.tgz\n",
            "Resolving archive.apache.org (archive.apache.org)... 65.108.204.189, 2a01:4f9:1a:a084::2\n",
            "Connecting to archive.apache.org (archive.apache.org)|65.108.204.189|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 71542357 (68M) [application/x-gzip]\n",
            "Saving to: ‘kafka_2.12-2.8.0.tgz.8’\n",
            "\n",
            "kafka_2.12-2.8.0.tg 100%[===================>]  68.23M  21.7MB/s    in 3.1s    \n",
            "\n",
            "2024-08-26 20:46:15 (21.7 MB/s) - ‘kafka_2.12-2.8.0.tgz.8’ saved [71542357/71542357]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import requests\n",
        "import os\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# Cria a pasta onde os arquivos serão salvos\n",
        "folder_path = \"dados_mercadobitcoin\"\n",
        "os.makedirs(folder_path, exist_ok=True)\n",
        "\n",
        "# Define a URL da API\n",
        "url = \"https://www.mercadobitcoin.net/api/BTC/ticker/\"\n",
        "\n",
        "\n",
        "# Define o tempo total (1 minuto) e o intervalo entre as requisições\n",
        "total_time = 30  # 1 minuto\n",
        "interval = 5  # intervalo entre as requisições (em segundos)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "while (time.time() - start_time) < total_time:\n",
        "    try:\n",
        "        # Faz a requisição para a API\n",
        "        response = requests.get(url)\n",
        "        data = response.json()\n",
        "\n",
        "        # Gera um timestamp para o nome do arquivo\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        file_name = f\"{timestamp}.json\"\n",
        "\n",
        "        # Salva os dados em um arquivo\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "        with open(file_path, 'w') as file:\n",
        "            json.dump(data, file)\n",
        "\n",
        "        print(f\"Dados salvos em {file_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao coletar dados: {e}\")\n",
        "\n",
        "    # Espera o tempo definido antes de fazer a próxima requisição\n",
        "    time.sleep(interval)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xqjMUYYUwZr",
        "outputId": "72741e6d-b40b-439c-ff56-9983081d77ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dados salvos em dados_mercadobitcoin/20240826_204623.json\n",
            "Dados salvos em dados_mercadobitcoin/20240826_204628.json\n",
            "Dados salvos em dados_mercadobitcoin/20240826_204633.json\n",
            "Dados salvos em dados_mercadobitcoin/20240826_204638.json\n",
            "Dados salvos em dados_mercadobitcoin/20240826_204643.json\n",
            "Dados salvos em dados_mercadobitcoin/20240826_204648.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from kafka import KafkaProducer\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Configurações do Kafka\n",
        "kafka_bootstrap_servers = 'localhost:9092'  # Altere conforme necessário\n",
        "kafka_topic = 'bitcoin_ticker'\n",
        "\n",
        "# Inicializa o produtor Kafka\n",
        "producer = KafkaProducer(bootstrap_servers=kafka_bootstrap_servers,\n",
        "                         value_serializer=lambda v: json.dumps(v).encode('utf-8'))\n",
        "\n",
        "# Caminho da pasta com os dados\n",
        "folder_path = \"dados_mercadobitcoin\"\n",
        "\n",
        "# Envia os arquivos JSON para o Kafka\n",
        "for file_name in os.listdir(folder_path):\n",
        "    if file_name.endswith('.json'):\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "        with open(file_path, 'r') as file:\n",
        "            data = json.load(file)\n",
        "            producer.send(kafka_topic, data)\n",
        "            print(f\"Dados do arquivo {file_name} enviados para o Kafka.\")\n",
        "\n",
        "producer.flush()\n",
        "producer.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdLdBO8QaqeI",
        "outputId": "36e22a47-ea8e-4f4f-c5c5-8e361ad13f19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dados do arquivo 20240826_194318.json enviados para o Kafka.\n",
            "Dados do arquivo 20240826_202052.json enviados para o Kafka.\n",
            "Dados do arquivo 20240826_204648.json enviados para o Kafka.\n",
            "Dados do arquivo 20240826_201705.json enviados para o Kafka.\n",
            "Dados do arquivo 20240826_204628.json enviados para o Kafka.\n",
            "Dados do arquivo 20240826_194638.json enviados para o Kafka.\n",
            "Dados do arquivo 20240826_204307.json enviados para o Kafka.\n",
            "Dados do arquivo 20240826_204638.json enviados para o Kafka.\n",
            "Dados do arquivo 20240826_204633.json enviados para o Kafka.\n",
            "Dados do arquivo 20240826_201720.json enviados para o Kafka.\n",
            "Dados do arquivo 20240826_195440.json enviados para o Kafka.\n",
            "Dados do arquivo 20240826_204317.json enviados para o Kafka.\n",
            "Dados do arquivo 20240826_204643.json enviados para o Kafka.\n",
            "Dados do arquivo 20240826_204623.json enviados para o Kafka.\n",
            "Dados do arquivo 20240826_202102.json enviados para o Kafka.\n",
            "Dados do arquivo 20240826_202057.json enviados para o Kafka.\n",
            "Dados do arquivo 20240826_193745.json enviados para o Kafka.\n",
            "Dados do arquivo 20240826_202041.json enviados para o Kafka.\n",
            "Dados do arquivo 20240826_195430.json enviados para o Kafka.\n",
            "Dados do arquivo 20240826_194313.json enviados para o Kafka.\n",
            "Dados do arquivo 20240826_194648.json enviados para o Kafka.\n",
            "Dados do arquivo 20240826_193719.json enviados para o Kafka.\n",
            "Dados do arquivo 20240826_194308.json enviados para o Kafka.\n",
            "Dados do arquivo 20240826_201725.json enviados para o Kafka.\n",
            "Dados do arquivo 20240826_194323.json enviados para o Kafka.\n",
            "Dados do arquivo 20240826_194214.json enviados para o Kafka.\n",
            "Dados do arquivo 20240826_194658.json enviados para o Kafka.\n",
            "Dados do arquivo 20240826_195435.json enviados para o Kafka.\n",
            "Dados do arquivo 20240826_194209.json enviados para o Kafka.\n",
            "Dados do arquivo 20240826_201715.json enviados para o Kafka.\n",
            "Dados do arquivo 20240826_195414.json enviados para o Kafka.\n",
            "Dados do arquivo 20240826_194333.json enviados para o Kafka.\n",
            "Dados do arquivo 20240826_201730.json enviados para o Kafka.\n",
            "Dados do arquivo 20240826_194328.json enviados para o Kafka.\n",
            "Dados do arquivo 20240826_201710.json enviados para o Kafka.\n",
            "Dados do arquivo 20240826_193735.json enviados para o Kafka.\n",
            "Dados do arquivo 20240826_202036.json enviados para o Kafka.\n",
            "Dados do arquivo 20240826_194653.json enviados para o Kafka.\n",
            "Dados do arquivo 20240826_195419.json enviados para o Kafka.\n",
            "Dados do arquivo 20240826_204312.json enviados para o Kafka.\n",
            "Dados do arquivo 20240826_193740.json enviados para o Kafka.\n",
            "Dados do arquivo 20240826_204322.json enviados para o Kafka.\n",
            "Dados do arquivo 20240826_204257.json enviados para o Kafka.\n",
            "Dados do arquivo 20240826_202046.json enviados para o Kafka.\n",
            "Dados do arquivo 20240826_194643.json enviados para o Kafka.\n",
            "Dados do arquivo 20240826_193724.json enviados para o Kafka.\n",
            "Dados do arquivo 20240826_193729.json enviados para o Kafka.\n",
            "Dados do arquivo 20240826_194632.json enviados para o Kafka.\n",
            "Dados do arquivo 20240826_195424.json enviados para o Kafka.\n",
            "Dados do arquivo 20240826_194204.json enviados para o Kafka.\n",
            "Dados do arquivo 20240826_204302.json enviados para o Kafka.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile consumidor.py\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import from_json, col\n",
        "from pyspark.sql.types import StructType, StructField, StringType, LongType\n",
        "\n",
        "# Cria a sessão do Spark\n",
        "spark = SparkSession.builder.appName(\"BitcoinTickerProcessor\") .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.0\").getOrCreate()\n",
        "\n",
        "# Definindo o schema para os dados JSON, com o campo \"data\" em vez de \"date\"\n",
        "schema = StructType([\n",
        "    StructField(\"ticker\", StructType([\n",
        "        StructField(\"high\", StringType(), True),\n",
        "        StructField(\"low\", StringType(), True),\n",
        "        StructField(\"vol\", StringType(), True),\n",
        "        StructField(\"last\", StringType(), True),\n",
        "        StructField(\"buy\", StringType(), True),\n",
        "        StructField(\"sell\", StringType(), True),\n",
        "        StructField(\"data\", LongType(), True)  # Campo alterado para \"data\"\n",
        "    ]))\n",
        "])\n",
        "\n",
        "# Lê os dados do Kafka\n",
        "df = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"localhost:9092\").option(\"subscribe\", \"bitcoin_ticker\").load()\n",
        "\n",
        "# Converte os dados de JSON para colunas\n",
        "df = df.selectExpr(\"CAST(value AS STRING)\") \\\n",
        "    .select(from_json(col(\"value\"), schema).alias(\"data\")) \\\n",
        "    .select(\"data.ticker.*\")\n",
        "\n",
        "# Configura o caminho de saída para os arquivos CSV\n",
        "output_path = \"/content/output_csv\"\n",
        "\n",
        "# Escreve o DataFrame processado em arquivos CSV\n",
        "query = df.writeStream \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .format(\"csv\") \\\n",
        "    .option(\"path\", output_path) \\\n",
        "    .option(\"checkpointLocation\", \"checkpoint/\") \\\n",
        "    .start()\n",
        "\n",
        "query.awaitTermination(10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2kQbFaDa6rE",
        "outputId": "7f2ffa2e-f9ec-4dea-97be-9d2bcfebc03e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting consumidor.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1 consumidor.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5-4LIrEh16i",
        "outputId": "fb71e9fa-4119-40f8-85fc-956339dde689"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ":: loading settings :: url = jar:file:/usr/local/lib/python3.10/dist-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
            "Ivy Default Cache set to: /root/.ivy2/cache\n",
            "The jars for the packages stored in: /root/.ivy2/jars\n",
            "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
            ":: resolving dependencies :: org.apache.spark#spark-submit-parent-1f9f4689-189e-4b7d-aca5-f29dbd7f8c5b;1.0\n",
            "\tconfs: [default]\n",
            "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.2.1 in central\n",
            "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.1 in central\n",
            "\tfound org.apache.kafka#kafka-clients;2.8.0 in central\n",
            "\tfound org.lz4#lz4-java;1.7.1 in central\n",
            "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
            "\tfound org.slf4j#slf4j-api;1.7.30 in central\n",
            "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.1 in central\n",
            "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
            "\tfound org.apache.hadoop#hadoop-client-api;3.3.1 in central\n",
            "\tfound org.apache.htrace#htrace-core4;4.1.0-incubating in central\n",
            "\tfound commons-logging#commons-logging;1.1.3 in central\n",
            "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
            "\tfound org.apache.commons#commons-pool2;2.6.2 in central\n",
            ":: resolution report :: resolve 1477ms :: artifacts dl 97ms\n",
            "\t:: modules in use:\n",
            "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
            "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
            "\torg.apache.commons#commons-pool2;2.6.2 from central in [default]\n",
            "\torg.apache.hadoop#hadoop-client-api;3.3.1 from central in [default]\n",
            "\torg.apache.hadoop#hadoop-client-runtime;3.3.1 from central in [default]\n",
            "\torg.apache.htrace#htrace-core4;4.1.0-incubating from central in [default]\n",
            "\torg.apache.kafka#kafka-clients;2.8.0 from central in [default]\n",
            "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.2.1 from central in [default]\n",
            "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.2.1 from central in [default]\n",
            "\torg.lz4#lz4-java;1.7.1 from central in [default]\n",
            "\torg.slf4j#slf4j-api;1.7.30 from central in [default]\n",
            "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
            "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
            "\t---------------------------------------------------------------------\n",
            "\t|                  |            modules            ||   artifacts   |\n",
            "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
            "\t---------------------------------------------------------------------\n",
            "\t|      default     |   13  |   0   |   0   |   0   ||   13  |   0   |\n",
            "\t---------------------------------------------------------------------\n",
            ":: retrieving :: org.apache.spark#spark-submit-parent-1f9f4689-189e-4b7d-aca5-f29dbd7f8c5b\n",
            "\tconfs: [default]\n",
            "\t0 artifacts copied, 13 already retrieved (0kB/48ms)\n",
            "24/08/26 20:46:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
            "24/08/26 20:47:02 INFO SparkContext: Running Spark version 3.5.2\n",
            "24/08/26 20:47:02 INFO SparkContext: OS info Linux, 6.1.85+, amd64\n",
            "24/08/26 20:47:02 INFO SparkContext: Java version 11.0.24\n",
            "24/08/26 20:47:02 INFO ResourceUtils: ==============================================================\n",
            "24/08/26 20:47:02 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
            "24/08/26 20:47:02 INFO ResourceUtils: ==============================================================\n",
            "24/08/26 20:47:02 INFO SparkContext: Submitted application: BitcoinTickerProcessor\n",
            "24/08/26 20:47:02 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
            "24/08/26 20:47:02 INFO ResourceProfile: Limiting resource is cpu\n",
            "24/08/26 20:47:02 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
            "24/08/26 20:47:02 INFO SecurityManager: Changing view acls to: root\n",
            "24/08/26 20:47:02 INFO SecurityManager: Changing modify acls to: root\n",
            "24/08/26 20:47:02 INFO SecurityManager: Changing view acls groups to: \n",
            "24/08/26 20:47:02 INFO SecurityManager: Changing modify acls groups to: \n",
            "24/08/26 20:47:02 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n",
            "24/08/26 20:47:03 INFO Utils: Successfully started service 'sparkDriver' on port 38225.\n",
            "24/08/26 20:47:03 INFO SparkEnv: Registering MapOutputTracker\n",
            "24/08/26 20:47:03 INFO SparkEnv: Registering BlockManagerMaster\n",
            "24/08/26 20:47:03 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
            "24/08/26 20:47:03 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
            "24/08/26 20:47:03 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
            "24/08/26 20:47:03 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-6ac022cb-79f8-4664-9145-b0e120d6451b\n",
            "24/08/26 20:47:03 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n",
            "24/08/26 20:47:03 INFO SparkEnv: Registering OutputCommitCoordinator\n",
            "24/08/26 20:47:04 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
            "24/08/26 20:47:04 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
            "24/08/26 20:47:04 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.2.1.jar at spark://a07effacf6e3:38225/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.2.1.jar with timestamp 1724705222618\n",
            "24/08/26 20:47:04 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.2.1.jar at spark://a07effacf6e3:38225/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.2.1.jar with timestamp 1724705222618\n",
            "24/08/26 20:47:04 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.0.jar at spark://a07effacf6e3:38225/jars/org.apache.kafka_kafka-clients-2.8.0.jar with timestamp 1724705222618\n",
            "24/08/26 20:47:04 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at spark://a07effacf6e3:38225/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1724705222618\n",
            "24/08/26 20:47:04 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.6.2.jar at spark://a07effacf6e3:38225/jars/org.apache.commons_commons-pool2-2.6.2.jar with timestamp 1724705222618\n",
            "24/08/26 20:47:04 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at spark://a07effacf6e3:38225/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1724705222618\n",
            "24/08/26 20:47:04 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.1.jar at spark://a07effacf6e3:38225/jars/org.apache.hadoop_hadoop-client-runtime-3.3.1.jar with timestamp 1724705222618\n",
            "24/08/26 20:47:04 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.lz4_lz4-java-1.7.1.jar at spark://a07effacf6e3:38225/jars/org.lz4_lz4-java-1.7.1.jar with timestamp 1724705222618\n",
            "24/08/26 20:47:04 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar at spark://a07effacf6e3:38225/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar with timestamp 1724705222618\n",
            "24/08/26 20:47:04 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.30.jar at spark://a07effacf6e3:38225/jars/org.slf4j_slf4j-api-1.7.30.jar with timestamp 1724705222618\n",
            "24/08/26 20:47:04 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.1.jar at spark://a07effacf6e3:38225/jars/org.apache.hadoop_hadoop-client-api-3.3.1.jar with timestamp 1724705222618\n",
            "24/08/26 20:47:04 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.htrace_htrace-core4-4.1.0-incubating.jar at spark://a07effacf6e3:38225/jars/org.apache.htrace_htrace-core4-4.1.0-incubating.jar with timestamp 1724705222618\n",
            "24/08/26 20:47:04 INFO SparkContext: Added JAR file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at spark://a07effacf6e3:38225/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1724705222618\n",
            "24/08/26 20:47:04 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.2.1.jar at file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.2.1.jar with timestamp 1724705222618\n",
            "24/08/26 20:47:04 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.2.1.jar to /tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/org.apache.spark_spark-sql-kafka-0-10_2.12-3.2.1.jar\n",
            "24/08/26 20:47:04 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.2.1.jar at file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.2.1.jar with timestamp 1724705222618\n",
            "24/08/26 20:47:04 INFO Utils: Copying /root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.2.1.jar to /tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.2.1.jar\n",
            "24/08/26 20:47:04 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.0.jar at file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.0.jar with timestamp 1724705222618\n",
            "24/08/26 20:47:04 INFO Utils: Copying /root/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.0.jar to /tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/org.apache.kafka_kafka-clients-2.8.0.jar\n",
            "24/08/26 20:47:04 INFO SparkContext: Added file file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar at file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1724705222618\n",
            "24/08/26 20:47:04 INFO Utils: Copying /root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/com.google.code.findbugs_jsr305-3.0.0.jar\n",
            "24/08/26 20:47:04 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.6.2.jar at file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.6.2.jar with timestamp 1724705222618\n",
            "24/08/26 20:47:04 INFO Utils: Copying /root/.ivy2/jars/org.apache.commons_commons-pool2-2.6.2.jar to /tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/org.apache.commons_commons-pool2-2.6.2.jar\n",
            "24/08/26 20:47:04 INFO SparkContext: Added file file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar at file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1724705222618\n",
            "24/08/26 20:47:04 INFO Utils: Copying /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/org.spark-project.spark_unused-1.0.0.jar\n",
            "24/08/26 20:47:04 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.1.jar at file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.1.jar with timestamp 1724705222618\n",
            "24/08/26 20:47:04 INFO Utils: Copying /root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.1.jar to /tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/org.apache.hadoop_hadoop-client-runtime-3.3.1.jar\n",
            "24/08/26 20:47:04 INFO SparkContext: Added file file:///root/.ivy2/jars/org.lz4_lz4-java-1.7.1.jar at file:///root/.ivy2/jars/org.lz4_lz4-java-1.7.1.jar with timestamp 1724705222618\n",
            "24/08/26 20:47:04 INFO Utils: Copying /root/.ivy2/jars/org.lz4_lz4-java-1.7.1.jar to /tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/org.lz4_lz4-java-1.7.1.jar\n",
            "24/08/26 20:47:04 INFO SparkContext: Added file file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar at file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar with timestamp 1724705222618\n",
            "24/08/26 20:47:04 INFO Utils: Copying /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar to /tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/org.xerial.snappy_snappy-java-1.1.8.4.jar\n",
            "24/08/26 20:47:04 INFO SparkContext: Added file file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.30.jar at file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.30.jar with timestamp 1724705222618\n",
            "24/08/26 20:47:04 INFO Utils: Copying /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.30.jar to /tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/org.slf4j_slf4j-api-1.7.30.jar\n",
            "24/08/26 20:47:04 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.1.jar at file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.1.jar with timestamp 1724705222618\n",
            "24/08/26 20:47:04 INFO Utils: Copying /root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.1.jar to /tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/org.apache.hadoop_hadoop-client-api-3.3.1.jar\n",
            "24/08/26 20:47:04 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.htrace_htrace-core4-4.1.0-incubating.jar at file:///root/.ivy2/jars/org.apache.htrace_htrace-core4-4.1.0-incubating.jar with timestamp 1724705222618\n",
            "24/08/26 20:47:04 INFO Utils: Copying /root/.ivy2/jars/org.apache.htrace_htrace-core4-4.1.0-incubating.jar to /tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/org.apache.htrace_htrace-core4-4.1.0-incubating.jar\n",
            "24/08/26 20:47:04 INFO SparkContext: Added file file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar at file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1724705222618\n",
            "24/08/26 20:47:04 INFO Utils: Copying /root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/commons-logging_commons-logging-1.1.3.jar\n",
            "24/08/26 20:47:04 INFO Executor: Starting executor ID driver on host a07effacf6e3\n",
            "24/08/26 20:47:04 INFO Executor: OS info Linux, 6.1.85+, amd64\n",
            "24/08/26 20:47:04 INFO Executor: Java version 11.0.24\n",
            "24/08/26 20:47:04 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
            "24/08/26 20:47:04 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@3e956f9f for default.\n",
            "24/08/26 20:47:04 INFO Executor: Fetching file:///root/.ivy2/jars/org.lz4_lz4-java-1.7.1.jar with timestamp 1724705222618\n",
            "24/08/26 20:47:04 INFO Utils: /root/.ivy2/jars/org.lz4_lz4-java-1.7.1.jar has been previously copied to /tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/org.lz4_lz4-java-1.7.1.jar\n",
            "24/08/26 20:47:04 INFO Executor: Fetching file:///root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar with timestamp 1724705222618\n",
            "24/08/26 20:47:04 INFO Utils: /root/.ivy2/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar has been previously copied to /tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/org.xerial.snappy_snappy-java-1.1.8.4.jar\n",
            "24/08/26 20:47:04 INFO Executor: Fetching file:///root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1724705222618\n",
            "24/08/26 20:47:04 INFO Utils: /root/.ivy2/jars/com.google.code.findbugs_jsr305-3.0.0.jar has been previously copied to /tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/com.google.code.findbugs_jsr305-3.0.0.jar\n",
            "24/08/26 20:47:04 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.0.jar with timestamp 1724705222618\n",
            "24/08/26 20:47:04 INFO Utils: /root/.ivy2/jars/org.apache.kafka_kafka-clients-2.8.0.jar has been previously copied to /tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/org.apache.kafka_kafka-clients-2.8.0.jar\n",
            "24/08/26 20:47:04 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.commons_commons-pool2-2.6.2.jar with timestamp 1724705222618\n",
            "24/08/26 20:47:04 INFO Utils: /root/.ivy2/jars/org.apache.commons_commons-pool2-2.6.2.jar has been previously copied to /tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/org.apache.commons_commons-pool2-2.6.2.jar\n",
            "24/08/26 20:47:04 INFO Executor: Fetching file:///root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1724705222618\n",
            "24/08/26 20:47:04 INFO Utils: /root/.ivy2/jars/commons-logging_commons-logging-1.1.3.jar has been previously copied to /tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/commons-logging_commons-logging-1.1.3.jar\n",
            "24/08/26 20:47:04 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.1.jar with timestamp 1724705222618\n",
            "24/08/26 20:47:04 INFO Utils: /root/.ivy2/jars/org.apache.hadoop_hadoop-client-runtime-3.3.1.jar has been previously copied to /tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/org.apache.hadoop_hadoop-client-runtime-3.3.1.jar\n",
            "24/08/26 20:47:04 INFO Executor: Fetching file:///root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1724705222618\n",
            "24/08/26 20:47:04 INFO Utils: /root/.ivy2/jars/org.spark-project.spark_unused-1.0.0.jar has been previously copied to /tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/org.spark-project.spark_unused-1.0.0.jar\n",
            "24/08/26 20:47:04 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.2.1.jar with timestamp 1724705222618\n",
            "24/08/26 20:47:04 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.2.1.jar has been previously copied to /tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.2.1.jar\n",
            "24/08/26 20:47:04 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.htrace_htrace-core4-4.1.0-incubating.jar with timestamp 1724705222618\n",
            "24/08/26 20:47:04 INFO Utils: /root/.ivy2/jars/org.apache.htrace_htrace-core4-4.1.0-incubating.jar has been previously copied to /tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/org.apache.htrace_htrace-core4-4.1.0-incubating.jar\n",
            "24/08/26 20:47:04 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.2.1.jar with timestamp 1724705222618\n",
            "24/08/26 20:47:04 INFO Utils: /root/.ivy2/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.2.1.jar has been previously copied to /tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/org.apache.spark_spark-sql-kafka-0-10_2.12-3.2.1.jar\n",
            "24/08/26 20:47:04 INFO Executor: Fetching file:///root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.1.jar with timestamp 1724705222618\n",
            "24/08/26 20:47:05 INFO Utils: /root/.ivy2/jars/org.apache.hadoop_hadoop-client-api-3.3.1.jar has been previously copied to /tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/org.apache.hadoop_hadoop-client-api-3.3.1.jar\n",
            "24/08/26 20:47:05 INFO Executor: Fetching file:///root/.ivy2/jars/org.slf4j_slf4j-api-1.7.30.jar with timestamp 1724705222618\n",
            "24/08/26 20:47:05 INFO Utils: /root/.ivy2/jars/org.slf4j_slf4j-api-1.7.30.jar has been previously copied to /tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/org.slf4j_slf4j-api-1.7.30.jar\n",
            "24/08/26 20:47:05 INFO Executor: Fetching spark://a07effacf6e3:38225/jars/org.apache.commons_commons-pool2-2.6.2.jar with timestamp 1724705222618\n",
            "24/08/26 20:47:05 INFO TransportClientFactory: Successfully created connection to a07effacf6e3/172.28.0.12:38225 after 40 ms (0 ms spent in bootstraps)\n",
            "24/08/26 20:47:05 INFO Utils: Fetching spark://a07effacf6e3:38225/jars/org.apache.commons_commons-pool2-2.6.2.jar to /tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/fetchFileTemp16727169309311851762.tmp\n",
            "24/08/26 20:47:05 INFO Utils: /tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/fetchFileTemp16727169309311851762.tmp has been previously copied to /tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/org.apache.commons_commons-pool2-2.6.2.jar\n",
            "24/08/26 20:47:05 INFO Executor: Adding file:/tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/org.apache.commons_commons-pool2-2.6.2.jar to class loader default\n",
            "24/08/26 20:47:05 INFO Executor: Fetching spark://a07effacf6e3:38225/jars/org.apache.htrace_htrace-core4-4.1.0-incubating.jar with timestamp 1724705222618\n",
            "24/08/26 20:47:05 INFO Utils: Fetching spark://a07effacf6e3:38225/jars/org.apache.htrace_htrace-core4-4.1.0-incubating.jar to /tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/fetchFileTemp11858702194937335208.tmp\n",
            "24/08/26 20:47:05 INFO Utils: /tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/fetchFileTemp11858702194937335208.tmp has been previously copied to /tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/org.apache.htrace_htrace-core4-4.1.0-incubating.jar\n",
            "24/08/26 20:47:05 INFO Executor: Adding file:/tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/org.apache.htrace_htrace-core4-4.1.0-incubating.jar to class loader default\n",
            "24/08/26 20:47:05 INFO Executor: Fetching spark://a07effacf6e3:38225/jars/com.google.code.findbugs_jsr305-3.0.0.jar with timestamp 1724705222618\n",
            "24/08/26 20:47:05 INFO Utils: Fetching spark://a07effacf6e3:38225/jars/com.google.code.findbugs_jsr305-3.0.0.jar to /tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/fetchFileTemp5620417408873312917.tmp\n",
            "24/08/26 20:47:05 INFO Utils: /tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/fetchFileTemp5620417408873312917.tmp has been previously copied to /tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/com.google.code.findbugs_jsr305-3.0.0.jar\n",
            "24/08/26 20:47:05 INFO Executor: Adding file:/tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/com.google.code.findbugs_jsr305-3.0.0.jar to class loader default\n",
            "24/08/26 20:47:05 INFO Executor: Fetching spark://a07effacf6e3:38225/jars/org.apache.kafka_kafka-clients-2.8.0.jar with timestamp 1724705222618\n",
            "24/08/26 20:47:05 INFO Utils: Fetching spark://a07effacf6e3:38225/jars/org.apache.kafka_kafka-clients-2.8.0.jar to /tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/fetchFileTemp13039132565804247820.tmp\n",
            "24/08/26 20:47:05 INFO Utils: /tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/fetchFileTemp13039132565804247820.tmp has been previously copied to /tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/org.apache.kafka_kafka-clients-2.8.0.jar\n",
            "24/08/26 20:47:05 INFO Executor: Adding file:/tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/org.apache.kafka_kafka-clients-2.8.0.jar to class loader default\n",
            "24/08/26 20:47:05 INFO Executor: Fetching spark://a07effacf6e3:38225/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.2.1.jar with timestamp 1724705222618\n",
            "24/08/26 20:47:05 INFO Utils: Fetching spark://a07effacf6e3:38225/jars/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.2.1.jar to /tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/fetchFileTemp11867016945033690181.tmp\n",
            "24/08/26 20:47:05 INFO Utils: /tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/fetchFileTemp11867016945033690181.tmp has been previously copied to /tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.2.1.jar\n",
            "24/08/26 20:47:05 INFO Executor: Adding file:/tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/org.apache.spark_spark-token-provider-kafka-0-10_2.12-3.2.1.jar to class loader default\n",
            "24/08/26 20:47:05 INFO Executor: Fetching spark://a07effacf6e3:38225/jars/org.spark-project.spark_unused-1.0.0.jar with timestamp 1724705222618\n",
            "24/08/26 20:47:05 INFO Utils: Fetching spark://a07effacf6e3:38225/jars/org.spark-project.spark_unused-1.0.0.jar to /tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/fetchFileTemp7785010227027415455.tmp\n",
            "24/08/26 20:47:05 INFO Utils: /tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/fetchFileTemp7785010227027415455.tmp has been previously copied to /tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/org.spark-project.spark_unused-1.0.0.jar\n",
            "24/08/26 20:47:05 INFO Executor: Adding file:/tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/org.spark-project.spark_unused-1.0.0.jar to class loader default\n",
            "24/08/26 20:47:05 INFO Executor: Fetching spark://a07effacf6e3:38225/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.2.1.jar with timestamp 1724705222618\n",
            "24/08/26 20:47:05 INFO Utils: Fetching spark://a07effacf6e3:38225/jars/org.apache.spark_spark-sql-kafka-0-10_2.12-3.2.1.jar to /tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/fetchFileTemp18396878462865631985.tmp\n",
            "24/08/26 20:47:05 INFO Utils: /tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/fetchFileTemp18396878462865631985.tmp has been previously copied to /tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/org.apache.spark_spark-sql-kafka-0-10_2.12-3.2.1.jar\n",
            "24/08/26 20:47:05 INFO Executor: Adding file:/tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/org.apache.spark_spark-sql-kafka-0-10_2.12-3.2.1.jar to class loader default\n",
            "24/08/26 20:47:05 INFO Executor: Fetching spark://a07effacf6e3:38225/jars/org.apache.hadoop_hadoop-client-runtime-3.3.1.jar with timestamp 1724705222618\n",
            "24/08/26 20:47:05 INFO Utils: Fetching spark://a07effacf6e3:38225/jars/org.apache.hadoop_hadoop-client-runtime-3.3.1.jar to /tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/fetchFileTemp5168044535169469414.tmp\n",
            "24/08/26 20:47:05 INFO Utils: /tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/fetchFileTemp5168044535169469414.tmp has been previously copied to /tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/org.apache.hadoop_hadoop-client-runtime-3.3.1.jar\n",
            "24/08/26 20:47:05 INFO Executor: Adding file:/tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/org.apache.hadoop_hadoop-client-runtime-3.3.1.jar to class loader default\n",
            "24/08/26 20:47:05 INFO Executor: Fetching spark://a07effacf6e3:38225/jars/org.lz4_lz4-java-1.7.1.jar with timestamp 1724705222618\n",
            "24/08/26 20:47:05 INFO Utils: Fetching spark://a07effacf6e3:38225/jars/org.lz4_lz4-java-1.7.1.jar to /tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/fetchFileTemp7369729202522117743.tmp\n",
            "24/08/26 20:47:05 INFO Utils: /tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/fetchFileTemp7369729202522117743.tmp has been previously copied to /tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/org.lz4_lz4-java-1.7.1.jar\n",
            "24/08/26 20:47:05 INFO Executor: Adding file:/tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/org.lz4_lz4-java-1.7.1.jar to class loader default\n",
            "24/08/26 20:47:05 INFO Executor: Fetching spark://a07effacf6e3:38225/jars/commons-logging_commons-logging-1.1.3.jar with timestamp 1724705222618\n",
            "24/08/26 20:47:05 INFO Utils: Fetching spark://a07effacf6e3:38225/jars/commons-logging_commons-logging-1.1.3.jar to /tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/fetchFileTemp10482826204350246767.tmp\n",
            "24/08/26 20:47:05 INFO Utils: /tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/fetchFileTemp10482826204350246767.tmp has been previously copied to /tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/commons-logging_commons-logging-1.1.3.jar\n",
            "24/08/26 20:47:05 INFO Executor: Adding file:/tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/commons-logging_commons-logging-1.1.3.jar to class loader default\n",
            "24/08/26 20:47:05 INFO Executor: Fetching spark://a07effacf6e3:38225/jars/org.apache.hadoop_hadoop-client-api-3.3.1.jar with timestamp 1724705222618\n",
            "24/08/26 20:47:05 INFO Utils: Fetching spark://a07effacf6e3:38225/jars/org.apache.hadoop_hadoop-client-api-3.3.1.jar to /tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/fetchFileTemp10256538951625271.tmp\n",
            "24/08/26 20:47:05 INFO Utils: /tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/fetchFileTemp10256538951625271.tmp has been previously copied to /tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/org.apache.hadoop_hadoop-client-api-3.3.1.jar\n",
            "24/08/26 20:47:05 INFO Executor: Adding file:/tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/org.apache.hadoop_hadoop-client-api-3.3.1.jar to class loader default\n",
            "24/08/26 20:47:05 INFO Executor: Fetching spark://a07effacf6e3:38225/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar with timestamp 1724705222618\n",
            "24/08/26 20:47:05 INFO Utils: Fetching spark://a07effacf6e3:38225/jars/org.xerial.snappy_snappy-java-1.1.8.4.jar to /tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/fetchFileTemp7025295993382131940.tmp\n",
            "24/08/26 20:47:05 INFO Utils: /tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/fetchFileTemp7025295993382131940.tmp has been previously copied to /tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/org.xerial.snappy_snappy-java-1.1.8.4.jar\n",
            "24/08/26 20:47:05 INFO Executor: Adding file:/tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/org.xerial.snappy_snappy-java-1.1.8.4.jar to class loader default\n",
            "24/08/26 20:47:05 INFO Executor: Fetching spark://a07effacf6e3:38225/jars/org.slf4j_slf4j-api-1.7.30.jar with timestamp 1724705222618\n",
            "24/08/26 20:47:05 INFO Utils: Fetching spark://a07effacf6e3:38225/jars/org.slf4j_slf4j-api-1.7.30.jar to /tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/fetchFileTemp8165305672434576622.tmp\n",
            "24/08/26 20:47:05 INFO Utils: /tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/fetchFileTemp8165305672434576622.tmp has been previously copied to /tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/org.slf4j_slf4j-api-1.7.30.jar\n",
            "24/08/26 20:47:05 INFO Executor: Adding file:/tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/userFiles-f7280047-8a7d-42b2-b45f-ce4018f9d3bf/org.slf4j_slf4j-api-1.7.30.jar to class loader default\n",
            "24/08/26 20:47:05 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39901.\n",
            "24/08/26 20:47:05 INFO NettyBlockTransferService: Server created on a07effacf6e3:39901\n",
            "24/08/26 20:47:05 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
            "24/08/26 20:47:05 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, a07effacf6e3, 39901, None)\n",
            "24/08/26 20:47:05 INFO BlockManagerMasterEndpoint: Registering block manager a07effacf6e3:39901 with 434.4 MiB RAM, BlockManagerId(driver, a07effacf6e3, 39901, None)\n",
            "24/08/26 20:47:05 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, a07effacf6e3, 39901, None)\n",
            "24/08/26 20:47:05 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, a07effacf6e3, 39901, None)\n",
            "24/08/26 20:47:06 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
            "24/08/26 20:47:06 INFO SharedState: Warehouse path is 'file:/content/spark-warehouse'.\n",
            "24/08/26 20:47:10 INFO StateStoreCoordinatorRef: Registered StateStoreCoordinator endpoint\n",
            "24/08/26 20:47:10 INFO ResolveWriteToStream: Checkpoint root checkpoint resolved to file:/content/checkpoint.\n",
            "24/08/26 20:47:10 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
            "24/08/26 20:47:10 INFO MicroBatchExecution: Starting [id = 8ec4dab6-5ea5-4b18-89a6-fe728a2fb1a3, runId = 817951b5-57ad-4a7c-b0cc-c9ee566673bf]. Use file:/content/checkpoint to store the query checkpoint.\n",
            "24/08/26 20:47:10 INFO MicroBatchExecution: Reading table [org.apache.spark.sql.kafka010.KafkaSourceProvider$KafkaTable@507a958e] from DataSourceV2 named 'kafka' [org.apache.spark.sql.kafka010.KafkaSourceProvider@2a2be161]\n",
            "24/08/26 20:47:11 INFO OffsetSeqLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6\n",
            "24/08/26 20:47:11 INFO OffsetSeqLog: Getting latest batch 6\n",
            "24/08/26 20:47:11 INFO OffsetSeqLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6\n",
            "24/08/26 20:47:11 INFO OffsetSeqLog: Getting latest batch 6\n",
            "24/08/26 20:47:11 INFO CommitLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6\n",
            "24/08/26 20:47:11 INFO CommitLog: Getting latest batch 6\n",
            "24/08/26 20:47:11 INFO MicroBatchExecution: Resuming at batch 7 with committed offsets {KafkaV2[Subscribe[bitcoin_ticker]]: {\"bitcoin_ticker\":{\"0\":186}}} and available offsets {KafkaV2[Subscribe[bitcoin_ticker]]: {\"bitcoin_ticker\":{\"0\":186}}}\n",
            "24/08/26 20:47:11 INFO MicroBatchExecution: Stream started from {KafkaV2[Subscribe[bitcoin_ticker]]: {\"bitcoin_ticker\":{\"0\":186}}}\n",
            "24/08/26 20:47:11 INFO AdminClientConfig: AdminClientConfig values: \n",
            "\tbootstrap.servers = [localhost:9092]\n",
            "\tclient.dns.lookup = use_all_dns_ips\n",
            "\tclient.id = \n",
            "\tconnections.max.idle.ms = 300000\n",
            "\tdefault.api.timeout.ms = 60000\n",
            "\tmetadata.max.age.ms = 300000\n",
            "\tmetric.reporters = []\n",
            "\tmetrics.num.samples = 2\n",
            "\tmetrics.recording.level = INFO\n",
            "\tmetrics.sample.window.ms = 30000\n",
            "\treceive.buffer.bytes = 65536\n",
            "\treconnect.backoff.max.ms = 1000\n",
            "\treconnect.backoff.ms = 50\n",
            "\trequest.timeout.ms = 30000\n",
            "\tretries = 2147483647\n",
            "\tretry.backoff.ms = 100\n",
            "\tsasl.client.callback.handler.class = null\n",
            "\tsasl.jaas.config = null\n",
            "\tsasl.kerberos.kinit.cmd = /usr/bin/kinit\n",
            "\tsasl.kerberos.min.time.before.relogin = 60000\n",
            "\tsasl.kerberos.service.name = null\n",
            "\tsasl.kerberos.ticket.renew.jitter = 0.05\n",
            "\tsasl.kerberos.ticket.renew.window.factor = 0.8\n",
            "\tsasl.login.callback.handler.class = null\n",
            "\tsasl.login.class = null\n",
            "\tsasl.login.refresh.buffer.seconds = 300\n",
            "\tsasl.login.refresh.min.period.seconds = 60\n",
            "\tsasl.login.refresh.window.factor = 0.8\n",
            "\tsasl.login.refresh.window.jitter = 0.05\n",
            "\tsasl.mechanism = GSSAPI\n",
            "\tsecurity.protocol = PLAINTEXT\n",
            "\tsecurity.providers = null\n",
            "\tsend.buffer.bytes = 131072\n",
            "\tsocket.connection.setup.timeout.max.ms = 30000\n",
            "\tsocket.connection.setup.timeout.ms = 10000\n",
            "\tssl.cipher.suites = null\n",
            "\tssl.enabled.protocols = [TLSv1.2, TLSv1.3]\n",
            "\tssl.endpoint.identification.algorithm = https\n",
            "\tssl.engine.factory.class = null\n",
            "\tssl.key.password = null\n",
            "\tssl.keymanager.algorithm = SunX509\n",
            "\tssl.keystore.certificate.chain = null\n",
            "\tssl.keystore.key = null\n",
            "\tssl.keystore.location = null\n",
            "\tssl.keystore.password = null\n",
            "\tssl.keystore.type = JKS\n",
            "\tssl.protocol = TLSv1.3\n",
            "\tssl.provider = null\n",
            "\tssl.secure.random.implementation = null\n",
            "\tssl.trustmanager.algorithm = PKIX\n",
            "\tssl.truststore.certificates = null\n",
            "\tssl.truststore.location = null\n",
            "\tssl.truststore.password = null\n",
            "\tssl.truststore.type = JKS\n",
            "\n",
            "24/08/26 20:47:11 WARN AdminClientConfig: The configuration 'key.deserializer' was supplied but isn't a known config.\n",
            "24/08/26 20:47:11 WARN AdminClientConfig: The configuration 'value.deserializer' was supplied but isn't a known config.\n",
            "24/08/26 20:47:11 WARN AdminClientConfig: The configuration 'enable.auto.commit' was supplied but isn't a known config.\n",
            "24/08/26 20:47:11 WARN AdminClientConfig: The configuration 'max.poll.records' was supplied but isn't a known config.\n",
            "24/08/26 20:47:11 WARN AdminClientConfig: The configuration 'auto.offset.reset' was supplied but isn't a known config.\n",
            "24/08/26 20:47:11 INFO AppInfoParser: Kafka version: 2.8.0\n",
            "24/08/26 20:47:11 INFO AppInfoParser: Kafka commitId: ebb1d6e21cc92130\n",
            "24/08/26 20:47:11 INFO AppInfoParser: Kafka startTimeMs: 1724705231518\n",
            "24/08/26 20:47:12 INFO CheckpointFileManager: Writing atomically to file:/content/checkpoint/offsets/7 using temp file file:/content/checkpoint/offsets/.7.5bbc246e-1e2d-46c7-9ed6-6e456b5023c3.tmp\n",
            "24/08/26 20:47:12 INFO CheckpointFileManager: Renamed temp file file:/content/checkpoint/offsets/.7.5bbc246e-1e2d-46c7-9ed6-6e456b5023c3.tmp to file:/content/checkpoint/offsets/7\n",
            "24/08/26 20:47:12 INFO MicroBatchExecution: Committed offsets for batch 7. Metadata OffsetSeqMetadata(0,1724705232447,Map(spark.sql.streaming.stateStore.providerClass -> org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider, spark.sql.streaming.join.stateFormatVersion -> 2, spark.sql.streaming.stateStore.compression.codec -> lz4, spark.sql.streaming.stateStore.rocksdb.formatVersion -> 5, spark.sql.streaming.statefulOperator.useStrictDistribution -> true, spark.sql.streaming.flatMapGroupsWithState.stateFormatVersion -> 2, spark.sql.streaming.multipleWatermarkPolicy -> min, spark.sql.streaming.aggregation.stateFormatVersion -> 2, spark.sql.shuffle.partitions -> 200))\n",
            "24/08/26 20:47:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
            "24/08/26 20:47:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
            "24/08/26 20:47:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
            "24/08/26 20:47:14 INFO KafkaOffsetReaderAdmin: Partitions added: Map()\n",
            "24/08/26 20:47:14 INFO FileStreamSinkLog: BatchIds found from listing: 0, 1, 2, 3, 4, 5, 6\n",
            "24/08/26 20:47:16 INFO CodeGenerator: Code generated in 438.359026 ms\n",
            "24/08/26 20:47:16 INFO SparkContext: Starting job: start at NativeMethodAccessorImpl.java:0\n",
            "24/08/26 20:47:16 INFO DAGScheduler: Got job 0 (start at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
            "24/08/26 20:47:16 INFO DAGScheduler: Final stage: ResultStage 0 (start at NativeMethodAccessorImpl.java:0)\n",
            "24/08/26 20:47:16 INFO DAGScheduler: Parents of final stage: List()\n",
            "24/08/26 20:47:16 INFO DAGScheduler: Missing parents: List()\n",
            "24/08/26 20:47:16 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at start at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
            "24/08/26 20:47:16 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 235.9 KiB, free 434.2 MiB)\n",
            "24/08/26 20:47:16 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 85.0 KiB, free 434.1 MiB)\n",
            "24/08/26 20:47:16 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on a07effacf6e3:39901 (size: 85.0 KiB, free: 434.3 MiB)\n",
            "24/08/26 20:47:16 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585\n",
            "24/08/26 20:47:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at start at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
            "24/08/26 20:47:17 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
            "24/08/26 20:47:17 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (a07effacf6e3, executor driver, partition 0, PROCESS_LOCAL, 12353 bytes) \n",
            "24/08/26 20:47:17 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
            "24/08/26 20:47:17 INFO CodeGenerator: Code generated in 114.805688 ms\n",
            "24/08/26 20:47:17 INFO CodeGenerator: Code generated in 51.124147 ms\n",
            "24/08/26 20:47:18 INFO CodeGenerator: Code generated in 57.728578 ms\n",
            "24/08/26 20:47:18 INFO CodeGenerator: Code generated in 46.119172 ms\n",
            "24/08/26 20:47:18 INFO ConsumerConfig: ConsumerConfig values: \n",
            "\tallow.auto.create.topics = true\n",
            "\tauto.commit.interval.ms = 5000\n",
            "\tauto.offset.reset = none\n",
            "\tbootstrap.servers = [localhost:9092]\n",
            "\tcheck.crcs = true\n",
            "\tclient.dns.lookup = use_all_dns_ips\n",
            "\tclient.id = consumer-spark-kafka-source-1dbd41a3-ef17-4ce9-9f50-a8feefd70f2d-488440281-executor-1\n",
            "\tclient.rack = \n",
            "\tconnections.max.idle.ms = 540000\n",
            "\tdefault.api.timeout.ms = 60000\n",
            "\tenable.auto.commit = false\n",
            "\texclude.internal.topics = true\n",
            "\tfetch.max.bytes = 52428800\n",
            "\tfetch.max.wait.ms = 500\n",
            "\tfetch.min.bytes = 1\n",
            "\tgroup.id = spark-kafka-source-1dbd41a3-ef17-4ce9-9f50-a8feefd70f2d-488440281-executor\n",
            "\tgroup.instance.id = null\n",
            "\theartbeat.interval.ms = 3000\n",
            "\tinterceptor.classes = []\n",
            "\tinternal.leave.group.on.close = true\n",
            "\tinternal.throw.on.fetch.stable.offset.unsupported = false\n",
            "\tisolation.level = read_uncommitted\n",
            "\tkey.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer\n",
            "\tmax.partition.fetch.bytes = 1048576\n",
            "\tmax.poll.interval.ms = 300000\n",
            "\tmax.poll.records = 500\n",
            "\tmetadata.max.age.ms = 300000\n",
            "\tmetric.reporters = []\n",
            "\tmetrics.num.samples = 2\n",
            "\tmetrics.recording.level = INFO\n",
            "\tmetrics.sample.window.ms = 30000\n",
            "\tpartition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor]\n",
            "\treceive.buffer.bytes = 65536\n",
            "\treconnect.backoff.max.ms = 1000\n",
            "\treconnect.backoff.ms = 50\n",
            "\trequest.timeout.ms = 30000\n",
            "\tretry.backoff.ms = 100\n",
            "\tsasl.client.callback.handler.class = null\n",
            "\tsasl.jaas.config = null\n",
            "\tsasl.kerberos.kinit.cmd = /usr/bin/kinit\n",
            "\tsasl.kerberos.min.time.before.relogin = 60000\n",
            "\tsasl.kerberos.service.name = null\n",
            "\tsasl.kerberos.ticket.renew.jitter = 0.05\n",
            "\tsasl.kerberos.ticket.renew.window.factor = 0.8\n",
            "\tsasl.login.callback.handler.class = null\n",
            "\tsasl.login.class = null\n",
            "\tsasl.login.refresh.buffer.seconds = 300\n",
            "\tsasl.login.refresh.min.period.seconds = 60\n",
            "\tsasl.login.refresh.window.factor = 0.8\n",
            "\tsasl.login.refresh.window.jitter = 0.05\n",
            "\tsasl.mechanism = GSSAPI\n",
            "\tsecurity.protocol = PLAINTEXT\n",
            "\tsecurity.providers = null\n",
            "\tsend.buffer.bytes = 131072\n",
            "\tsession.timeout.ms = 10000\n",
            "\tsocket.connection.setup.timeout.max.ms = 30000\n",
            "\tsocket.connection.setup.timeout.ms = 10000\n",
            "\tssl.cipher.suites = null\n",
            "\tssl.enabled.protocols = [TLSv1.2, TLSv1.3]\n",
            "\tssl.endpoint.identification.algorithm = https\n",
            "\tssl.engine.factory.class = null\n",
            "\tssl.key.password = null\n",
            "\tssl.keymanager.algorithm = SunX509\n",
            "\tssl.keystore.certificate.chain = null\n",
            "\tssl.keystore.key = null\n",
            "\tssl.keystore.location = null\n",
            "\tssl.keystore.password = null\n",
            "\tssl.keystore.type = JKS\n",
            "\tssl.protocol = TLSv1.3\n",
            "\tssl.provider = null\n",
            "\tssl.secure.random.implementation = null\n",
            "\tssl.trustmanager.algorithm = PKIX\n",
            "\tssl.truststore.certificates = null\n",
            "\tssl.truststore.location = null\n",
            "\tssl.truststore.password = null\n",
            "\tssl.truststore.type = JKS\n",
            "\tvalue.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer\n",
            "\n",
            "24/08/26 20:47:18 INFO AppInfoParser: Kafka version: 2.8.0\n",
            "24/08/26 20:47:18 INFO AppInfoParser: Kafka commitId: ebb1d6e21cc92130\n",
            "24/08/26 20:47:18 INFO AppInfoParser: Kafka startTimeMs: 1724705238218\n",
            "24/08/26 20:47:18 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1dbd41a3-ef17-4ce9-9f50-a8feefd70f2d-488440281-executor-1, groupId=spark-kafka-source-1dbd41a3-ef17-4ce9-9f50-a8feefd70f2d-488440281-executor] Subscribed to partition(s): bitcoin_ticker-0\n",
            "24/08/26 20:47:18 INFO KafkaConsumer: [Consumer clientId=consumer-spark-kafka-source-1dbd41a3-ef17-4ce9-9f50-a8feefd70f2d-488440281-executor-1, groupId=spark-kafka-source-1dbd41a3-ef17-4ce9-9f50-a8feefd70f2d-488440281-executor] Seeking to offset 186 for partition bitcoin_ticker-0\n",
            "24/08/26 20:47:18 INFO Metadata: [Consumer clientId=consumer-spark-kafka-source-1dbd41a3-ef17-4ce9-9f50-a8feefd70f2d-488440281-executor-1, groupId=spark-kafka-source-1dbd41a3-ef17-4ce9-9f50-a8feefd70f2d-488440281-executor] Cluster ID: vu9muxY4Q-KUTSCmUYGBNA\n",
            "24/08/26 20:47:18 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1dbd41a3-ef17-4ce9-9f50-a8feefd70f2d-488440281-executor-1, groupId=spark-kafka-source-1dbd41a3-ef17-4ce9-9f50-a8feefd70f2d-488440281-executor] Seeking to EARLIEST offset of partition bitcoin_ticker-0\n",
            "24/08/26 20:47:18 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1dbd41a3-ef17-4ce9-9f50-a8feefd70f2d-488440281-executor-1, groupId=spark-kafka-source-1dbd41a3-ef17-4ce9-9f50-a8feefd70f2d-488440281-executor] Resetting offset for partition bitcoin_ticker-0 to position FetchPosition{offset=0, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[a07effacf6e3:9092 (id: 0 rack: null)], epoch=0}}.\n",
            "24/08/26 20:47:18 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1dbd41a3-ef17-4ce9-9f50-a8feefd70f2d-488440281-executor-1, groupId=spark-kafka-source-1dbd41a3-ef17-4ce9-9f50-a8feefd70f2d-488440281-executor] Seeking to LATEST offset of partition bitcoin_ticker-0\n",
            "24/08/26 20:47:18 INFO SubscriptionState: [Consumer clientId=consumer-spark-kafka-source-1dbd41a3-ef17-4ce9-9f50-a8feefd70f2d-488440281-executor-1, groupId=spark-kafka-source-1dbd41a3-ef17-4ce9-9f50-a8feefd70f2d-488440281-executor] Resetting offset for partition bitcoin_ticker-0 to position FetchPosition{offset=237, offsetEpoch=Optional.empty, currentLeader=LeaderAndEpoch{leader=Optional[a07effacf6e3:9092 (id: 0 rack: null)], epoch=0}}.\n",
            "24/08/26 20:47:19 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 3089 bytes result sent to driver\n",
            "24/08/26 20:47:19 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2236 ms on a07effacf6e3 (executor driver) (1/1)\n",
            "24/08/26 20:47:19 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
            "24/08/26 20:47:19 INFO DAGScheduler: ResultStage 0 (start at NativeMethodAccessorImpl.java:0) finished in 2.776 s\n",
            "24/08/26 20:47:19 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
            "24/08/26 20:47:19 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
            "24/08/26 20:47:19 INFO DAGScheduler: Job 0 finished: start at NativeMethodAccessorImpl.java:0, took 2.860552 s\n",
            "24/08/26 20:47:19 INFO FileFormatWriter: Start to commit write Job cee06772-a715-4e30-85a3-348c9e2cf246.\n",
            "24/08/26 20:47:19 INFO FileStreamSinkLog: Set the compact interval to 10 [defaultCompactInterval: 10]\n",
            "24/08/26 20:47:19 INFO CheckpointFileManager: Writing atomically to /content/output_csv/_spark_metadata/7 using temp file /content/output_csv/_spark_metadata/.7.b46aec75-f3c5-4e2d-874f-aeb5059d0c1b.tmp\n",
            "24/08/26 20:47:19 INFO CheckpointFileManager: Renamed temp file /content/output_csv/_spark_metadata/.7.b46aec75-f3c5-4e2d-874f-aeb5059d0c1b.tmp to /content/output_csv/_spark_metadata/7\n",
            "24/08/26 20:47:19 INFO ManifestFileCommitProtocol: Committed batch 7\n",
            "24/08/26 20:47:19 INFO FileFormatWriter: Write Job cee06772-a715-4e30-85a3-348c9e2cf246 committed. Elapsed time: 102 ms.\n",
            "24/08/26 20:47:19 INFO FileFormatWriter: Finished processing stats for write job cee06772-a715-4e30-85a3-348c9e2cf246.\n",
            "24/08/26 20:47:19 INFO CheckpointFileManager: Writing atomically to file:/content/checkpoint/commits/7 using temp file file:/content/checkpoint/commits/.7.8a68d2d2-cbc9-4581-acb0-50037210a5b6.tmp\n",
            "24/08/26 20:47:19 INFO CheckpointFileManager: Renamed temp file file:/content/checkpoint/commits/.7.8a68d2d2-cbc9-4581-acb0-50037210a5b6.tmp to file:/content/checkpoint/commits/7\n",
            "24/08/26 20:47:19 INFO MicroBatchExecution: Streaming query made progress: {\n",
            "  \"id\" : \"8ec4dab6-5ea5-4b18-89a6-fe728a2fb1a3\",\n",
            "  \"runId\" : \"817951b5-57ad-4a7c-b0cc-c9ee566673bf\",\n",
            "  \"name\" : null,\n",
            "  \"timestamp\" : \"2024-08-26T20:47:10.987Z\",\n",
            "  \"batchId\" : 7,\n",
            "  \"numInputRows\" : 51,\n",
            "  \"inputRowsPerSecond\" : 0.0,\n",
            "  \"processedRowsPerSecond\" : 5.913043478260869,\n",
            "  \"durationMs\" : {\n",
            "    \"addBatch\" : 5027,\n",
            "    \"commitOffsets\" : 69,\n",
            "    \"getBatch\" : 23,\n",
            "    \"latestOffset\" : 1316,\n",
            "    \"queryPlanning\" : 1792,\n",
            "    \"triggerExecution\" : 8623,\n",
            "    \"walCommit\" : 199\n",
            "  },\n",
            "  \"stateOperators\" : [ ],\n",
            "  \"sources\" : [ {\n",
            "    \"description\" : \"KafkaV2[Subscribe[bitcoin_ticker]]\",\n",
            "    \"startOffset\" : {\n",
            "      \"bitcoin_ticker\" : {\n",
            "        \"0\" : 186\n",
            "      }\n",
            "    },\n",
            "    \"endOffset\" : {\n",
            "      \"bitcoin_ticker\" : {\n",
            "        \"0\" : 237\n",
            "      }\n",
            "    },\n",
            "    \"latestOffset\" : {\n",
            "      \"bitcoin_ticker\" : {\n",
            "        \"0\" : 237\n",
            "      }\n",
            "    },\n",
            "    \"numInputRows\" : 51,\n",
            "    \"inputRowsPerSecond\" : 0.0,\n",
            "    \"processedRowsPerSecond\" : 5.913043478260869,\n",
            "    \"metrics\" : {\n",
            "      \"avgOffsetsBehindLatest\" : \"0.0\",\n",
            "      \"maxOffsetsBehindLatest\" : \"0\",\n",
            "      \"minOffsetsBehindLatest\" : \"0\"\n",
            "    }\n",
            "  } ],\n",
            "  \"sink\" : {\n",
            "    \"description\" : \"FileSink[/content/output_csv]\",\n",
            "    \"numOutputRows\" : -1\n",
            "  }\n",
            "}\n",
            "24/08/26 20:47:21 INFO Metrics: Metrics scheduler closed\n",
            "24/08/26 20:47:21 INFO Metrics: Closing reporter org.apache.kafka.common.metrics.JmxReporter\n",
            "24/08/26 20:47:21 INFO Metrics: Metrics reporters closed\n",
            "24/08/26 20:47:21 INFO AppInfoParser: App info kafka.consumer for consumer-spark-kafka-source-1dbd41a3-ef17-4ce9-9f50-a8feefd70f2d-488440281-executor-1 unregistered\n",
            "24/08/26 20:47:21 INFO SparkContext: Invoking stop() from shutdown hook\n",
            "24/08/26 20:47:21 INFO SparkContext: SparkContext is stopping with exitCode 0.\n",
            "24/08/26 20:47:21 INFO SparkUI: Stopped Spark web UI at http://a07effacf6e3:4040\n",
            "24/08/26 20:47:21 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n",
            "24/08/26 20:47:21 INFO MemoryStore: MemoryStore cleared\n",
            "24/08/26 20:47:21 INFO BlockManager: BlockManager stopped\n",
            "24/08/26 20:47:21 INFO BlockManagerMaster: BlockManagerMaster stopped\n",
            "24/08/26 20:47:21 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n",
            "24/08/26 20:47:21 INFO SparkContext: Successfully stopped SparkContext\n",
            "24/08/26 20:47:21 INFO ShutdownHookManager: Shutdown hook called\n",
            "24/08/26 20:47:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-7f1b5350-517b-472c-8420-74fc7f894768\n",
            "24/08/26 20:47:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4\n",
            "24/08/26 20:47:21 INFO ShutdownHookManager: Deleting directory /tmp/spark-b1339faf-de3d-4e63-9e46-1052187e86b4/pyspark-84cc55ae-d9e9-4357-ab6f-e307ada69f89\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MongoDB download and installation\n",
        "!wget https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-3.0.15.tgz  # Downloads MongoDB from official repository\n",
        "!tar xfv /content/mongodb-linux-x86_64-3.0.15.tgz    # Unpack compressed file\n",
        "#!rm mongodb-linux-x86_64-debian71-3.0.15.tgz          # Removes downloaded file\n",
        "\n",
        "# Default location of database is \"/data/db\" folder\n",
        "!mkdir /data                                          # data folder creation\n",
        "!mkdir /data/db"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSEhV4wMoSrM",
        "outputId": "2705da71-90db-47da-cb7f-5fdd1be9ecd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-08-26 20:47:21--  https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-3.0.15.tgz\n",
            "Resolving fastdl.mongodb.org (fastdl.mongodb.org)... 18.160.18.101, 18.160.18.6, 18.160.18.8, ...\n",
            "Connecting to fastdl.mongodb.org (fastdl.mongodb.org)|18.160.18.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 63276568 (60M) [application/x-gzip]\n",
            "Saving to: ‘mongodb-linux-x86_64-3.0.15.tgz.7’\n",
            "\n",
            "\r          mongodb-l   0%[                    ]       0  --.-KB/s               \r         mongodb-li  89%[================>   ]  53.83M   269MB/s               \rmongodb-linux-x86_6 100%[===================>]  60.34M   279MB/s    in 0.2s    \n",
            "\n",
            "2024-08-26 20:47:21 (279 MB/s) - ‘mongodb-linux-x86_64-3.0.15.tgz.7’ saved [63276568/63276568]\n",
            "\n",
            "mongodb-linux-x86_64-3.0.15/README\n",
            "mongodb-linux-x86_64-3.0.15/THIRD-PARTY-NOTICES\n",
            "mongodb-linux-x86_64-3.0.15/GNU-AGPL-3.0\n",
            "mongodb-linux-x86_64-3.0.15/bin/mongodump\n",
            "mongodb-linux-x86_64-3.0.15/bin/mongorestore\n",
            "mongodb-linux-x86_64-3.0.15/bin/mongoexport\n",
            "mongodb-linux-x86_64-3.0.15/bin/mongoimport\n",
            "mongodb-linux-x86_64-3.0.15/bin/mongostat\n",
            "mongodb-linux-x86_64-3.0.15/bin/mongotop\n",
            "mongodb-linux-x86_64-3.0.15/bin/bsondump\n",
            "mongodb-linux-x86_64-3.0.15/bin/mongofiles\n",
            "mongodb-linux-x86_64-3.0.15/bin/mongooplog\n",
            "mongodb-linux-x86_64-3.0.15/bin/mongoperf\n",
            "mongodb-linux-x86_64-3.0.15/bin/mongod\n",
            "mongodb-linux-x86_64-3.0.15/bin/mongos\n",
            "mongodb-linux-x86_64-3.0.15/bin/mongo\n",
            "mkdir: cannot create directory ‘/data’: File exists\n",
            "mkdir: cannot create directory ‘/data/db’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "11# Runs mongoDB server\n",
        "#!mongodb-linux-x86_64-debian71-3.0.15/bin/mongod --nojournal --dbpath GDrive/My\\ Drive/data/db\n",
        "!nohup /content/mongodb-linux-x86_64-3.0.15/bin/mongod --nojournal --dbpath /data/db >log1 &\n",
        "\n",
        "#import subprocess\n",
        "#subprocess.Popen([\"./mongodb-linux-x86_64-debian71-3.0.15/bin/mongod\",\" --nojournal --dbpath /data/db\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3ux2buWoVz8",
        "outputId": "40a78794-2c46-4b23-dcc4-4ead5baf4147"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: redirecting stderr to stdout\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymongo==3.10.0 pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X09XH_t8ohbM",
        "outputId": "4b8ce4db-d8a3-471a-cbcd-bf2455424302"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pymongo==3.10.0 in /usr/local/lib/python3.10/dist-packages (3.10.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Nome das colunas\n",
        "columns = ['high', 'low', 'vol', 'last', 'buy', 'sell', 'data']\n",
        "\n",
        "# Caminho da pasta contendo os arquivos CSV\n",
        "folder_path = '/content/output_csv/'\n",
        "\n",
        "# Iterar sobre todos os arquivos na pasta\n",
        "for filename in os.listdir(folder_path):\n",
        "    if filename.endswith('.csv'):\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "\n",
        "        # Ler o arquivo CSV\n",
        "        df = pd.read_csv(file_path, header=None)\n",
        "\n",
        "        # Atribuir os nomes das colunas\n",
        "        df.columns = columns\n",
        "\n",
        "        # Salvar o arquivo CSV atualizado\n",
        "        df.to_csv(file_path, index=False)\n",
        "\n",
        "        print(f\"Arquivo atualizado: {filename}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpSzOIJLa_hd",
        "outputId": "7c5c7184-ec28-451c-8dec-c847fb5861fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arquivo atualizado: part-00000-b2f20a26-95a8-41cd-8b7f-25e2218b8356-c000.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pymongo import MongoClient\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# Definir o caminho da pasta contendo arquivos CSV\n",
        "csv_folder_path = '/content/output_csv/'\n",
        "\n",
        "# Passo 3: Iniciar uma sessão PySpark\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"CSV to MongoDB\") \\\n",
        "    .config(\"spark.mongodb.input.uri\", \"mongodb://127.0.0.1/yourdb.yourcollection\") \\\n",
        "    .config(\"spark.mongodb.output.uri\", \"mongodb://127.0.0.1/yourdb.yourcollection\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Passo 4: Obter a lista de arquivos CSV na pasta\n",
        "csv_files = glob.glob(os.path.join(csv_folder_path, '*.csv'))\n",
        "\n",
        "if not csv_files:\n",
        "    raise FileNotFoundError(f\"Nenhum arquivo CSV encontrado na pasta: {csv_folder_path}\")\n",
        "\n",
        "# Passo 5: Conectar ao MongoDB\n",
        "client = MongoClient('mongodb://localhost:27017/')\n",
        "db = client['yourdb']\n",
        "collection = db['yourcollection']\n",
        "\n",
        "# Função para substituir pontos nas chaves do dicionário\n",
        "def replace_dot_keys(data):\n",
        "    if isinstance(data, dict):\n",
        "        return {k.replace('.', '_'): replace_dot_keys(v) for k, v in data.items()}\n",
        "    elif isinstance(data, list):\n",
        "        return [replace_dot_keys(item) for item in data]\n",
        "    else:\n",
        "        return data\n",
        "\n",
        "# Processar cada arquivo CSV\n",
        "for csv_file in csv_files:\n",
        "    print(f\"Processando arquivo: {csv_file}\")\n",
        "\n",
        "    # Ler o arquivo CSV usando PySpark\n",
        "    df = spark.read.csv(csv_file, header=True, inferSchema=True)\n",
        "    df.show()\n",
        "\n",
        "    # Converter o DataFrame do Spark para uma lista de dicionários\n",
        "    data = df.toPandas().to_dict(orient='records')\n",
        "\n",
        "    # Substituir pontos nas chaves dos dicionários\n",
        "    data = replace_dot_keys(data)\n",
        "\n",
        "    # Inserir os dados no MongoDB\n",
        "    collection.insert_many(data)\n",
        "\n",
        "# Verificar os dados inseridos\n",
        "inserted_data = collection.find()\n",
        "for document in inserted_data:\n",
        "    print(document)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "o09nT9dTXecr",
        "outputId": "d7f6cdfa-f25d-48a5-dd48-5597fa6e6aba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processando arquivo: /content/output_csv/part-00000-b2f20a26-95a8-41cd-8b7f-25e2218b8356-c000.csv\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o31.csv.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (a07effacf6e3 executor driver): org.apache.spark.SparkException: Encountered error while reading file file:///content/output_csv/part-00000-b2f20a26-95a8-41cd-8b7f-25e2218b8356-c000.csv. Details:\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:864)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:293)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.hadoop.fs.ChecksumException: Checksum error: file:/content/output_csv/part-00000-b2f20a26-95a8-41cd-8b7f-25e2218b8356-c000.csv at 0 exp: -210322563 got: 1874713976\n\tat org.apache.hadoop.fs.FSInputChecker.verifySums(FSInputChecker.java:347)\n\tat org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:303)\n\tat org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)\n\tat org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)\n\tat java.base/java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\n\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\n\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n\t... 22 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4334)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4324)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4322)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4322)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3316)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3539)\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:111)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:64)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:62)\n\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Encountered error while reading file file:///content/output_csv/part-00000-b2f20a26-95a8-41cd-8b7f-25e2218b8356-c000.csv. Details:\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:864)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:293)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: org.apache.hadoop.fs.ChecksumException: Checksum error: file:/content/output_csv/part-00000-b2f20a26-95a8-41cd-8b7f-25e2218b8356-c000.csv at 0 exp: -210322563 got: 1874713976\n\tat org.apache.hadoop.fs.FSInputChecker.verifySums(FSInputChecker.java:347)\n\tat org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:303)\n\tat org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)\n\tat org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)\n\tat java.base/java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\n\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\n\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n\t... 22 more\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-8e6decd4a66f>\u001b[0m in \u001b[0;36m<cell line: 37>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m# Ler o arquivo CSV usando PySpark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o31.csv.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (a07effacf6e3 executor driver): org.apache.spark.SparkException: Encountered error while reading file file:///content/output_csv/part-00000-b2f20a26-95a8-41cd-8b7f-25e2218b8356-c000.csv. Details:\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:864)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:293)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.hadoop.fs.ChecksumException: Checksum error: file:/content/output_csv/part-00000-b2f20a26-95a8-41cd-8b7f-25e2218b8356-c000.csv at 0 exp: -210322563 got: 1874713976\n\tat org.apache.hadoop.fs.FSInputChecker.verifySums(FSInputChecker.java:347)\n\tat org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:303)\n\tat org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)\n\tat org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)\n\tat java.base/java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\n\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\n\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n\t... 22 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4334)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4324)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4322)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4322)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3316)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3539)\n\tat org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:111)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:64)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:62)\n\tat org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)\n\tat scala.Option.orElse(Option.scala:447)\n\tat org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Encountered error while reading file file:///content/output_csv/part-00000-b2f20a26-95a8-41cd-8b7f-25e2218b8356-c000.csv. Details:\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:864)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:293)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\t... 1 more\nCaused by: org.apache.hadoop.fs.ChecksumException: Checksum error: file:/content/output_csv/part-00000-b2f20a26-95a8-41cd-8b7f-25e2218b8356-c000.csv at 0 exp: -210322563 got: 1874713976\n\tat org.apache.hadoop.fs.FSInputChecker.verifySums(FSInputChecker.java:347)\n\tat org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:303)\n\tat org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:252)\n\tat org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:197)\n\tat java.base/java.io.DataInputStream.read(DataInputStream.java:149)\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)\n\tat org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)\n\tat org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)\n\tat org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)\n\tat org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n\tat org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n\t... 22 more\n"
          ]
        }
      ]
    }
  ]
}